# BERT-for-Burmese-Text-Classification-
Myanmar Text Classification 
BERT, which stands for Bidirectional Encoder Representations from Transformers, is a deep learning model based on Transformers. In 2018, Google created this algorithm that enhances the ability to understand the context of unlabeled text for various tasks. It achieves this by predicting text that might come before and after (bi-directional) other text.


https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages
I. Explanatory Data Analysis<br>
II. Data Preprocessing<br>
III. Building the Model<br>
IV. Compiling the Model<br>
V. Model Evaluation<br>
VI. Classifying Movie Reviews

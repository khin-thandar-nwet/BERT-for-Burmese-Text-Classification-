# BERT-for-Burmese-Text-Classification-
Myanmar Text Classification 
BERT, which stands for Bidirectional Encoder Representations from Transformers, is a deep learning model based on Transformers. In 2018, Google created this algorithm that enhances the ability to understand the context of unlabeled text for various tasks. It achieves this by predicting text that might come before and after (bi-directional) other text.


https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages
I. Explanatory Data Analysis
II. Data Preprocessing
III. Building the Model
IV. Compiling the Model
V. Model Evaluation
VI. Classifying Movie Reviews
